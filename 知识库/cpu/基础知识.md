![[进程状态.png]]
### TLB miss

CPU访问某个虚拟内存地址的过程如下

- 1.CPU产生一个虚拟地址
    
- 2.MMU从TLB中获取页表，翻译成物理地址
    
- 3.MMU把物理地址发送给L1/L2/L3/内存
    
- 4.L1/L2/L3/内存将地址对应数据返回给CPU


> [!NOTE] 为什么需要TLB
> 64位系统的虚拟内存实现：四级页表
> 一次内存io请求就需要四次页表的访问io+真正的内存访问io

> [!NOTE] 使用大内存页
> 因为TLB并不是很大，只有4k，而且现在逻辑核又造成会有两个进程来共享。所以可能会有cache miss的情况出现。而且一旦TLB miss造成的后果可比物理地址cache miss后果要严重一些，最多可能需要进行5次内存IO才行。建议你先用上面的perf工具查看一下你的程序的TLB的miss情况，如果确实不命中率很高，那么Linux允许你使用大内存页

## 内核cpu开销
#### 进程/线程切换的开销(上下文切换)
1. 切换页表全局目录
2. 切换内核态堆栈
3. 切换硬件上下文（寄存器相关数据）
4. 刷新TLB
5. 系统调度器代码执行

> [!NOTE] 间接开销
> 跨cpu调度导致新进程穿透到内存的io会变多


> [!NOTE] 查看上下文开销的命令
> vmstat -1
> sar -w -1

#### 系统调用的开销
1. 切换到同进程的内核态上下文
2. 进入内核态继续工作

#### 软中断的开销
1. 切换到了另外一个内核进程ksoftirqd上
2. 进入内核态继续工作

> [!NOTE] 软中断开销与系统调用快笑
> Contents
